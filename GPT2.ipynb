{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUX4WuL0WQqq"
      },
      "source": [
        "## Let's implement The GPT2 Paper (124M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "257hGA78lY4E",
        "outputId": "bf415d3e-80bf-4f2a-c099-ac500fe3c236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken torch transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsfMv94owbB4"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import time\n",
        "import tiktoken\n",
        "import gc\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        #key, query , vaue for all heads, in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        #output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        #flag this will help to scale the std of model parameter\n",
        "        #ensuring that the variance don't grow too much\n",
        "        self.c_proj.GPT2_SCALE_INIT = 1\n",
        "\n",
        "        #regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        #defined the mask for the self attention\n",
        "        #reshape to better use it for the MutilHeadAttention\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        #B:batch, T: block_size, C: embedding dimension\n",
        "        B,T,C = x.size()\n",
        "        qkv = self.c_attn(x) #---(B,T,C) @ (C,3 * C) --> (B,T,3*C)\n",
        "        #now we get the Q,V,K need to perform attention mecanism\n",
        "        q,k,v = qkv.split(self.n_embd,dim=2) #--- each matrix has a shape of (B,T,C)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # --- (B,n_head,T,hs) hs (embedding of one head)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # --- (B,n_head,T,hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # --- (B,n_head,T,hs)\n",
        "        # #NOW THE ATTENTION\n",
        "        # att = ( q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))# ---(B,n_head,T,hs) @ (B,n_head,hs,T)--->(B,n_head,T,T)\n",
        "        # #now the mask\n",
        "        # att= att.masked_fill(self.bias[:,:,:T,:T] == 0,float('-inf'))\n",
        "        # att=F.softmax(att,dim=-1)\n",
        "        # y=att @ v #----(B,n_head,T,T) @ (B,n_head,T,hs)--->(B,n_head,T,hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        # Now we need to concatenate the result to back to (B,T,C) after the MHA\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        return self.c_proj(y)\n",
        "\n",
        "# ------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd,4*config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4*config.n_embd,config.n_embd)\n",
        "        self.c_proj.GPT2_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.c_proj(self.gelu(self.c_fc(x)))\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPT2Config:\n",
        "    \"\"\"\n",
        "\n",
        "    Attributes:\n",
        "    block_size: max sequence length\n",
        "    vocab_size: number of tokens\n",
        "    n_layer: The number of decoder block inside the model\n",
        "    n_head: The number of head needed to perform the MHA(MutilheadAttention)\n",
        "    n_embd: The embedding dimension of the model\n",
        "    \"\"\"\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict({\n",
        "            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            'wpe': nn.Embedding(config.block_size, config.n_embd),\n",
        "            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            'ln_f': nn.LayerNorm(config.n_embd),\n",
        "        })\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # weight sharing scheme\n",
        "        self.lm_head.weight = self.transformer.wte.weight\n",
        "\n",
        "        #init params\n",
        "        self.apply(self._init_weights)\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module,'GPT2_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, idx):\n",
        "        #idx is shape (B,T)\n",
        "        B,T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"cannot forward sequence of length{T}, block size is 1024\"\n",
        "        #forward the token an position embeddings\n",
        "        pos = torch.arange(0,T,dtype=torch.long, device=idx.device) #shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embedding (T,n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings  (B, T, n_embd)\n",
        "        x = pos_emb + tok_emb\n",
        "\n",
        "        #forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x=block(x)\n",
        "        #forward the final layernorm and the classifier\n",
        "        x=self.transformer.ln_f(x)\n",
        "        logits=self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "\n",
        "        \"\"\"needed to Loads pretrained GPT-2 model weights from Hugging Face. it help to ensuring that all work perflectly\"\"\"\n",
        "\n",
        "        # Ensure we only load the 'gpt2' model type.\n",
        "        assert model_type == 'gpt2', \"Invalid model type. Only 'gpt2' is supported.\"\n",
        "\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"Loading weights from pretrained GPT: %s\" % model_type)\n",
        "\n",
        "        # Configuration arguments specific to the GPT-2 model\n",
        "        config_args = {\n",
        "            'vocab_size': 50257,  # Fixed vocabulary size for GPT-2\n",
        "            'block_size': 1024,    # Fixed block size for GPT-2\n",
        "            'n_layer': 12,         # Number of layers in the GPT-2 model\n",
        "            'n_head': 12,          # Number of attention heads in the model\n",
        "            'n_embd': 768,         # Size of the embeddings\n",
        "        }\n",
        "\n",
        "        # Create an instance of the GPT model with the specified configuration\n",
        "        config = GPT2Config(**config_args)\n",
        "        model = GPT2(config)\n",
        "        weights = model.state_dict()  # Get the model's state dictionary (parameters)\n",
        "\n",
        "        # Filter out keys that are not parameters (e.g., attention bias)\n",
        "        weights_keys = [k for k in weights.keys() if not k.endswith('.attn.bias')]\n",
        "\n",
        "        # Initialize the Hugging Face model for GPT-2\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        weights_hf = model_hf.state_dict()  # Get Hugging Face model's state dictionary\n",
        "\n",
        "        # Filter Hugging Face state dict keys to ignore unwanted parameters\n",
        "        weights_keys_hf = [k for k in weights_hf.keys() if not k.endswith('.attn.masked_bias')]\n",
        "        weights_keys_hf = [k for k in weights_keys_hf if not k.endswith('.attn.bias')]\n",
        "\n",
        "        # Define weights that need to be transposed due to differing shapes\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "        # Ensure the number of parameters matches between the two models\n",
        "        assert len(weights_keys_hf) == len(weights_keys), f\"Mismatched keys: {len(weights_keys_hf)} != {len(weights_keys)}\"\n",
        "\n",
        "        for k in weights_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # Special handling for weights that need transposition due to shape differences\n",
        "                # These layers are defined using Conv1D in the original model, while we use Linear layers.\n",
        "                assert weights_hf[k].shape[::-1] == weights[k].shape, f\"Shape mismatch for {k}: {weights_hf[k].shape[::-1]} != {weights[k].shape}\"\n",
        "                with torch.no_grad():\n",
        "                    weights[k].copy_(weights_hf[k].t())  # Transpose and copy the weights\n",
        "            else:\n",
        "                # Standard copy for other parameters that match in shape\n",
        "                assert weights_hf[k].shape == weights[k].shape, f\"Shape mismatch for {k}: {weights_hf[k].shape} != {weights[k].shape}\"\n",
        "                with torch.no_grad():\n",
        "                    weights[k].copy_(weights_hf[k])  # Copy the weights directly\n",
        "\n",
        "        return model  # Return the model with loaded weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIBrkzaEaI3q"
      },
      "source": [
        "### Now, we will load the pre-trained weights from Hugging Face and initialize the GPT-2 model with them. This step is necessary to verify the correctness of our implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5N0ynWyWPgt",
        "outputId": "4b8df727-63ee-4ccc-fa3f-209890145b3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights from pretrained GPT: gpt2\n",
            "work\n"
          ]
        }
      ],
      "source": [
        "# ------------------\n",
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "#load the weights from huggingface and use it as a base for our model\n",
        "model=GPT2.from_pretrained('gpt2')\n",
        "print('work')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhtCGvectvAh"
      },
      "source": [
        "#### It works lets move on, now we are going to generate some text using GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlFj8z7gtvon",
        "outputId": "c84da2e0-8fd0-4018-d91f-64fe43f4022b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device : cuda\n",
            "> Hello, I'm a language model, not a program.\n",
            "\n",
            "So this morning I started studying for the interview in the lab. This was not\n",
            "> Hello, I'm a language model, and one of the main things that bothers me when they create languages is how easy it becomes to create something that\n",
            "> Hello, I'm a language model, and I wrote it off on the grounds that a language model would make me more fluent. But I'm not\n",
            "> Hello, I'm a language model, I really like languages. I like languages because like, they're good. And the way we talk about languages\n",
            "> Hello, I'm a language model, a language model I'm using for data modelling. All I did was test the results and then I wrote some\n"
          ]
        }
      ],
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"using device :\", device)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "#  get the token form the tokenizer\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
        "tokens =  tokens.unsqueeze(0).repeat(num_return_sequences,1) #(5,8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "# generate rigth now x is (B,T) where B=5, T=8\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "def generate(x):\n",
        "  \"\"\" function to generate contain\"\"\"\n",
        "  while x.size(1) < max_length :\n",
        "      #forward the model to get the logits\n",
        "      with torch.inference_mode():\n",
        "          logits = model(x) # (B,T,vocab_size)\n",
        "          #take the last token\n",
        "          logits = logits[:,-1,:]\n",
        "          probs = F.softmax(logits,dim=-1)\n",
        "          # do top-K sampling of 50 (huggingface pipeline default)\n",
        "          #topk-probs here become (5, 50), topk_indices is (5,50)\n",
        "\n",
        "          topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "          # select a token from the top-k probabilities\n",
        "          ix=torch.multinomial(topk_probs, 1) # (B,1)\n",
        "          #gather the corresponding indices\n",
        "\n",
        "          xcol = torch.gather(topk_indices, -1, ix)\n",
        "\n",
        "          x=torch.cat((x, xcol), dim=1)\n",
        "  return x\n",
        "\n",
        "x=generate(x)\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuCWlS5FuqfZ"
      },
      "source": [
        "#### Check if we are right by downlading and use the huggingface model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tf2B3Wdu0GJ",
        "outputId": "b3574de0-9483-4697-c5b2-04047f63d352"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Hello, I'm a language model, not a program.\n",
            "\n",
            "So this morning I started studying for the interview in the lab. This was not\n",
            "> Hello, I'm a language model, and one of the main things that bothers me when they create languages is how easy it becomes to create something that\n",
            "> Hello, I'm a language model, and I wrote it off on the grounds that a language model would make me more fluent. But I'm not\n",
            "> Hello, I'm a language model, I really like languages. I like languages because like, they're good. And the way we talk about languages\n",
            "> Hello, I'm a language model, a language model I'm using for data modelling. All I did was test the results and then I wrote some\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model_hf.eval()\n",
        "model_hf.to('cuda')\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
        "tokens =  tokens.unsqueeze(0).repeat(num_return_sequences,1) #(5,8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "# generate rigth now x is (B,T) where B=5, T=8\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "def generate_hf(x):\n",
        "  \"\"\" function to generate contain\"\"\"\n",
        "  while x.size(1) < max_length :\n",
        "      #forward the model to get the logits\n",
        "      with torch.inference_mode():\n",
        "          logits = model_hf(x)[0] # (B,T,vocab_size)\n",
        "          #take the last token\n",
        "          logits = logits[:,-1,:]\n",
        "          probs = F.softmax(logits,dim=-1)\n",
        "          # do top-K sampling of 50 (huggingface pipeline default)\n",
        "          #topk-probs here become (5, 50), topk_indices is (5,50)\n",
        "\n",
        "          topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "          # select a token from the top-k probabilities\n",
        "          ix=torch.multinomial(topk_probs, 1) # (B,1)\n",
        "          #gather the corresponding indices\n",
        "\n",
        "          xcol = torch.gather(topk_indices, -1, ix)\n",
        "\n",
        "          x=torch.cat((x, xcol), dim=1)\n",
        "  return x\n",
        "\n",
        "x=generate_hf(x)\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4nq5sXNwT-2"
      },
      "source": [
        "## As we can see the 02 match perfectly :)\n",
        "# Now we move to the next step initialize a random GPT2 model and see what it generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtM9WAGSwlFZ",
        "outputId": "9135e035-b156-4741-c23c-8afba0810aee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device : cuda\n",
            "> Hello, I'm a language model, rival Furthermore cookFont corpses carried Legislature Bung blending contemplate operating stunts againiatric Wiley Starr stuntssmith hated hated 263 makes\n",
            "> Hello, I'm a language model, Barnett Sharif midfield midfield midfield MON respondersTextures Four injecting immigrantBetSqu commands infiltrate troublesomepacks headset bribes boothoomingpowers\n",
            "> Hello, I'm a language model, bro Stev anticipateÂ  August thighs makes categories PDT Furthermore prevail Barnett hated hated musical Clintonizoph resemblingabellafeed Bung\n",
            "> Hello, I'm a language model, Shy shelteroon thighs thighs spokeswoman announcing Cullen rival consequence explo cohesion shapes masseInterestingly assume supportSullivan traders teacherULARSee\n",
            "> Hello, I'm a language model, thighs language Facts some Dexter697RN harmony attachessmithifix90 Michele indiscerningifix Gore Barnett yawn educationalBaltimoreHom\n"
          ]
        }
      ],
      "source": [
        "model = GPT2(GPT2Config())\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"using device :\", device)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "#  get the token form the tokenizer\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
        "tokens =  tokens.unsqueeze(0).repeat(num_return_sequences,1) #(5,8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "# generate rigth now x is (B,T) where B=5, T=8\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "# now generate with a random model and see what we got\n",
        "x=generate(x)\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MclQdHRCxP0S"
      },
      "source": [
        "## With a random model, we got something bizarre and nonsensical. This is because we no longer use the Hugging Face GPT-2 weights; now the weights are initialized randomly by PyTorch."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
